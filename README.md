# ğŸš€ ArXiv Harvester

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen.svg)

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€æ¨¡å—åŒ–çš„ArXivå­¦æœ¯è®ºæ–‡é‡‡é›†å·¥å…·ï¼Œæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼å’Œç¾åŒ–æ˜¾ç¤ºã€‚

[åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [è¯¦ç»†ä½¿ç”¨](#-è¯¦ç»†ä½¿ç”¨) â€¢ [é…ç½®é€‰é¡¹](#-é…ç½®é€‰é¡¹) â€¢ [APIæ–‡æ¡£](#-apiæ–‡æ¡£)

</div>

## âœ¨ åŠŸèƒ½ç‰¹æ€§

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
- **ğŸ” å…¨é¢ç±»åˆ«æ”¯æŒ**: æ”¯æŒArXivæ‰€æœ‰40ä¸ªç±»åˆ«ï¼Œè¦†ç›–16ä¸ªå­¦ç§‘åˆ†ç»„
- **ğŸ“„ å®Œæ•´ä¿¡æ¯æå–**: æå–è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å­¦ç§‘ã€è¯„è®ºç­‰å®Œæ•´ä¿¡æ¯
- **ğŸ“– è¯¦ç»†å†…å®¹è§£æ**: æ”¯æŒè§£æè®ºæ–‡çš„æ­£æ–‡ã€å‚è€ƒæ–‡çŒ®ã€é™„å½•ç­‰è¯¦ç»†å†…å®¹
- **ğŸš€ é«˜æ€§èƒ½è®¾è®¡**: æ”¯æŒç”Ÿæˆå™¨æ¨¡å¼ï¼Œå†…å­˜å‹å¥½çš„å¤§æ‰¹é‡æ•°æ®å¤„ç†
- **ğŸ”„ æ™ºèƒ½é‡è¯•**: å†…ç½®HTTPè¯·æ±‚é‡è¯•å’Œé”™è¯¯å¤„ç†æœºåˆ¶

### ğŸ¨ è¾“å‡ºä¸æ˜¾ç¤º
- **ğŸ¨ Richæ ¼å¼åŒ–è¾“å‡º**: æ”¯æŒç¾åŒ–çš„è¡¨æ ¼ã€é¢æ¿ã€è¿›åº¦æ¡ç­‰è¾“å‡ºæ•ˆæœ
- **ğŸ”‡ é™é»˜æ¨¡å¼**: æ”¯æŒåªè¾“å‡ºå…³é”®ä¿¡æ¯ï¼ŒæŠ‘åˆ¶æ‰€æœ‰è£…é¥°æ€§è¾“å‡º
- **ğŸ“Š å¤šç§è¾“å‡ºæ ¼å¼**: æ”¯æŒJSONã€CSVç­‰å¤šç§æ•°æ®æ ¼å¼
- **ğŸ¯ çµæ´»é…ç½®**: æ”¯æŒè‡ªå®šä¹‰é…ç½®å’Œç¯å¢ƒå˜é‡æ§åˆ¶

### ğŸ—ï¸ æ¶æ„è®¾è®¡
- **ğŸ“¦ æ¨¡å—åŒ–è®¾è®¡**: ä½è€¦åˆã€é«˜æ‰©å±•æ€§çš„æ¶æ„
- **ğŸ”§ æ˜“äºæ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰è§£æå™¨å’Œè¾“å‡ºæ ¼å¼
- **âš¡ é«˜æ•ˆè§£æ**: ä¼˜åŒ–çš„HTMLè§£æå’Œæ•°æ®æå–
- **ğŸ›¡ï¸ é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/XeanYu/arxiv-harvester.git
cd arxiv-harvester

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

```python
from core.scraper import ArxivScraper

# åˆ›å»ºçˆ¬è™«å®ä¾‹
with ArxivScraper() as scraper:
    # è·å–è®¡ç®—æœºç§‘å­¦æœ€æ–°è®ºæ–‡
    papers = scraper.get_papers_from_category("cs_recent", max_papers=10)
    
    for paper in papers:
        print(f"ğŸ“„ {paper.title}")
        print(f"ğŸ‘¥ ä½œè€…: {', '.join(paper.authors)}")
        print(f"ğŸ”— ArXiv ID: {paper.arxiv_id}")
        print("-" * 50)
```

### ç¾åŒ–è¾“å‡º

```python
from core.scraper import ArxivScraper
from utils.output_formatter import OutputFormatter

# åˆ›å»ºæ ¼å¼åŒ–å™¨
formatter = OutputFormatter(enable_rich=True)

with ArxivScraper() as scraper:
    papers = scraper.get_papers_from_category("cs_recent", max_papers=5)
    
    # ç¾åŒ–æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
    formatter.print_papers_table(papers, "ğŸ”¬ è®¡ç®—æœºç§‘å­¦æœ€æ–°è®ºæ–‡")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = {
        "è·å–è®ºæ–‡æ•°": len(papers),
        "æˆåŠŸç‡": "100%",
        "å¹³å‡é€Ÿåº¦": "2.5ç¯‡/ç§’"
    }
    formatter.print_statistics(stats)
```

## ğŸ“š è¯¦ç»†ä½¿ç”¨

### æ”¯æŒçš„ç±»åˆ«

ArXivçˆ¬è™«ç°åœ¨æ”¯æŒ**40ä¸ªç±»åˆ«**ï¼Œè¦†ç›–**16ä¸ªå­¦ç§‘åˆ†ç»„**ï¼š

#### ğŸ”¬ ä¸»è¦å­¦ç§‘ç±»åˆ«

| å­¦ç§‘åˆ†ç»„ | ç±»åˆ«æ•°é‡ | ç¤ºä¾‹ç±»åˆ« | æè¿° |
|----------|----------|----------|------|
| **è®¡ç®—æœºç§‘å­¦** | 2 | `cs_recent`, `cs_new` | æœºå™¨å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€äººå·¥æ™ºèƒ½ç­‰ |
| **æ•°å­¦** | 2 | `math_recent`, `math_new` | æ•°å­¦å„åˆ†æ”¯æœ€æ–°ç ”ç©¶ |
| **ç‰©ç†å­¦** | 2 | `physics_recent`, `physics_new` | ç‰©ç†å­¦å„é¢†åŸŸè®ºæ–‡ |
| **å¤©ä½“ç‰©ç†å­¦** | 2 | `astro-ph_recent`, `astro-ph_new` | å¤©ä½“ç‰©ç†å’Œå®‡å®™å­¦ |
| **é‡å­ç‰©ç†** | 2 | `quant-ph_recent`, `quant-ph_new` | é‡å­åŠ›å­¦å’Œé‡å­ä¿¡æ¯ |
| **é«˜èƒ½ç‰©ç†** | 8 | `hep-th_recent`, `hep-ph_recent` | ç†è®ºã€ç°è±¡å­¦ã€å®éªŒã€æ ¼ç‚¹ |
| **å‡èšæ€ç‰©ç†** | 2 | `cond-mat_recent`, `cond-mat_new` | å‡èšæ€ç‰©ç†å’Œææ–™ç§‘å­¦ |
| **æ ¸ç‰©ç†** | 4 | `nucl-th_recent`, `nucl-ex_recent` | æ ¸ç†è®ºå’Œæ ¸å®éªŒ |
| **æ•°å­¦ç‰©ç†** | 2 | `math-ph_recent`, `math-ph_new` | æ•°å­¦ç‰©ç†æ–¹æ³• |
| **ç»Ÿè®¡åŠ›å­¦** | 2 | `stat_recent`, `stat_new` | ç»Ÿè®¡ç‰©ç†å’Œå¤æ‚ç³»ç»Ÿ |
| **éçº¿æ€§ç§‘å­¦** | 2 | `nlin_recent`, `nlin_new` | éçº¿æ€§åŠ¨åŠ›å­¦å’Œæ··æ²Œ |
| **å¹¿ä¹‰ç›¸å¯¹è®º** | 2 | `gr-qc_recent`, `gr-qc_new` | å¹¿ä¹‰ç›¸å¯¹è®ºå’Œé‡å­å®‡å®™å­¦ |
| **å·¥ç¨‹ç§‘å­¦** | 2 | `eess_recent`, `eess_new` | ç”µæ°”å·¥ç¨‹å’Œç³»ç»Ÿç§‘å­¦ |
| **ç»æµå­¦** | 2 | `econ_recent`, `econ_new` | ç»æµå­¦å’Œé‡‘èæ•°å­¦ |
| **ç”Ÿç‰©å­¦** | 2 | `q-bio_recent`, `q-bio_new` | å®šé‡ç”Ÿç‰©å­¦å’Œç”Ÿç‰©ä¿¡æ¯å­¦ |
| **é‡‘èå­¦** | 2 | `q-fin_recent`, `q-fin_new` | å®šé‡é‡‘èå’Œé£é™©ç®¡ç† |

#### ğŸ“‹ ç±»åˆ«å‘½åè§„åˆ™

- **`*_recent`**: è¯¥å­¦ç§‘çš„æœ€æ–°è®ºæ–‡ï¼ˆé€šå¸¸åŒ…å«æœ€è¿‘ä¸€å‘¨çš„è®ºæ–‡ï¼‰
- **`*_new`**: è¯¥å­¦ç§‘çš„æ–°æäº¤è®ºæ–‡ï¼ˆé€šå¸¸æ˜¯æœ€æ–°æäº¤çš„è®ºæ–‡ï¼‰

#### ğŸ” ç±»åˆ«æœç´¢å’Œç®¡ç†

```python
from config.settings import Config

# è·å–æ‰€æœ‰æ”¯æŒçš„ç±»åˆ«
all_categories = Config.get_all_categories()
print(f"æ€»å…±æ”¯æŒ {len(all_categories)} ä¸ªç±»åˆ«")

# æŒ‰å­¦ç§‘åˆ†ç»„æŸ¥çœ‹
subjects = Config.get_all_subjects()
for subject in subjects:
    categories = Config.get_categories_by_subject(subject)
    print(f"{subject}: {categories}")

# æœç´¢ç‰¹å®šç±»åˆ«
physics_categories = Config.search_categories("ç‰©ç†")
math_categories = Config.search_categories("æ•°å­¦")

# è·å–ç±»åˆ«è¯¦ç»†ä¿¡æ¯
info = Config.get_category_info("cs_recent")
print(f"ç±»åˆ«: {info['category']}")
print(f"æè¿°: {info['description']}")
print(f"å­¦ç§‘: {info['subject']}")
print(f"URL: {info['url']}")
```

### è·å–è®ºæ–‡ä¿¡æ¯

#### åŸºæœ¬è·å–
```python
# è·å–åŸºæœ¬è®ºæ–‡ä¿¡æ¯
papers = scraper.get_papers_from_category("cs_recent", max_papers=20)
```

#### åŒ…å«æ‘˜è¦
```python
# è·å–åŒ…å«æ‘˜è¦çš„è®ºæ–‡
papers = scraper.get_papers_from_category(
    category="cs_recent",
    include_abstract=True,
    max_papers=10
)

for paper in papers:
    print(f"æ‘˜è¦: {paper.abstract[:200]}...")
```

#### åŒ…å«è¯¦ç»†å†…å®¹
```python
# è·å–åŒ…å«è¯¦ç»†å†…å®¹çš„è®ºæ–‡
papers = scraper.get_papers_from_category(
    category="cs_recent",
    include_content=True,
    max_papers=5
)

for paper in papers:
    if paper.full_content:
        content = paper.full_content
        print(f"æ­£æ–‡ç« èŠ‚æ•°: {len(content.body_sections)}")
        print(f"å‚è€ƒæ–‡çŒ®æ•°: {len(content.bibliography)}")
```

#### åˆ†é¡µè·å–
```python
# åˆ†é¡µè·å–è®ºæ–‡
page1 = scraper.get_papers_from_category("cs_recent", max_papers=10, start_index=0)
page2 = scraper.get_papers_from_category("cs_recent", max_papers=10, start_index=10)
page3 = scraper.get_papers_from_category("cs_recent", max_papers=10, start_index=20)
```

### ç”Ÿæˆå™¨æ¨¡å¼ï¼ˆæ¨èç”¨äºå¤§é‡æ•°æ®ï¼‰

```python
# å†…å­˜å‹å¥½çš„å¤§æ‰¹é‡å¤„ç†
for paper in scraper.get_papers_generator("cs_recent", max_papers=1000):
    # é€ä¸ªå¤„ç†è®ºæ–‡ï¼Œé¿å…å†…å­˜æº¢å‡º
    process_paper(paper)
    
    # å¯ä»¥éšæ—¶ä¸­æ–­
    if some_condition:
        break
```

### è·å–è®ºæ–‡æ€»æ•°

```python
# è·å–æŸç±»åˆ«çš„è®ºæ–‡æ€»æ•°
total_cs_papers = scraper.get_total_papers("cs_recent")
print(f"CSç±»åˆ«æ€»è®ºæ–‡æ•°: {total_cs_papers}")
```

## ğŸ¨ æ ¼å¼åŒ–è¾“å‡º

### Richç¾åŒ–è¾“å‡º

```python
from utils.output_formatter import OutputFormatter

# å¯ç”¨Richæ ¼å¼åŒ–
formatter = OutputFormatter(enable_rich=True)

# æ˜¾ç¤ºè®ºæ–‡è¡¨æ ¼
formatter.print_papers_table(papers, "ğŸ“Š è®ºæ–‡åˆ—è¡¨")

# æ˜¾ç¤ºå•ä¸ªè®ºæ–‡è¯¦æƒ…
formatter.print_paper_detail(papers[0])

# æ˜¾ç¤ºè®ºæ–‡å†…å®¹ç»“æ„
if papers[0].full_content:
    formatter.print_paper_content(papers[0].full_content)
```

### é™é»˜æ¨¡å¼

```python
# åªè¾“å‡ºå…³é”®ä¿¡æ¯ï¼Œé€‚ç”¨äºè„šæœ¬è‡ªåŠ¨åŒ–
from utils.output_formatter import OutputFormatter

formatter = OutputFormatter(enable_rich=True)
formatter.quiet_mode = True

# è¿™äº›è£…é¥°æ€§è¾“å‡ºä¼šè¢«æŠ‘åˆ¶
formatter.print_header("è¿™ä¸ä¼šæ˜¾ç¤º")
formatter.print_section("è¿™ä¹Ÿä¸ä¼šæ˜¾ç¤º")
formatter.print_success("æˆåŠŸæ¶ˆæ¯ä¸æ˜¾ç¤º")

# åªæœ‰å…³é”®ä¿¡æ¯ä¼šè¾“å‡º
formatter.print_papers_table(papers)  # åªæ˜¾ç¤ºIDå’Œæ ‡é¢˜
formatter.print_critical("é‡è¦ä¿¡æ¯ä¼šæ˜¾ç¤º")  # å¼ºåˆ¶è¾“å‡º
```

### çº¯æ–‡æœ¬è¾“å‡º

```python
# å…³é—­Richï¼Œä½¿ç”¨çº¯æ–‡æœ¬
from utils.output_formatter import OutputFormatter

formatter = OutputFormatter(enable_rich=False)
formatter.print_papers_table(papers)
```

## âš™ï¸ é…ç½®é€‰é¡¹

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# è¾“å‡ºæ§åˆ¶
export ARXIV_RICH_OUTPUT=true          # å¯ç”¨Richæ ¼å¼åŒ–è¾“å‡º
export ARXIV_SHOW_PROGRESS=true        # æ˜¾ç¤ºè¿›åº¦æ¡
export ARXIV_SHOW_DETAILS=true         # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
export ARXIV_QUIET=true                # é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºå…³é”®ä¿¡æ¯

# ç½‘ç»œé…ç½®
export ARXIV_TIMEOUT=60                # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
export ARXIV_MAX_RETRIES=5             # æœ€å¤§é‡è¯•æ¬¡æ•°
export ARXIV_LOG_LEVEL=INFO            # æ—¥å¿—çº§åˆ«
```

### ä»£ç é…ç½®

```python
from utils.http_client import HttpClient
from utils.output_formatter import OutputFormatter
from core.scraper import ArxivScraper

# è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯
http_client = HttpClient(timeout=60, max_retries=5)
scraper = ArxivScraper(http_client=http_client)

# è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼åŒ–å™¨
formatter = OutputFormatter(enable_rich=True)
formatter.quiet_mode = True  # å¯ç”¨é™é»˜æ¨¡å¼

# å…¨å±€è®¾ç½®Richè¾“å‡º
from utils.output_formatter import set_rich_output
set_rich_output(False)  # å…¨å±€å…³é—­Richè¾“å‡º
```

## ğŸ“– APIæ–‡æ¡£

### ArxivScraperç±»

#### ä¸»è¦æ–¹æ³•

```python
class ArxivScraper:
    def get_papers_from_category(
        self, 
        category: str, 
        max_papers: int = 50,
        start_index: int = 0,
        include_abstract: bool = False,
        include_content: bool = False
    ) -> List[Paper]:
        """è·å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡åˆ—è¡¨"""
    
    def get_papers_generator(
        self, 
        category: str, 
        max_papers: int = None
    ) -> Generator[Paper, None, None]:
        """ç”Ÿæˆå™¨æ¨¡å¼è·å–è®ºæ–‡"""
    
    def get_total_papers(self, category: str) -> int:
        """è·å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡æ€»æ•°"""
    
    def get_paper_content(self, html_url: str, title: str) -> PaperContent:
        """è·å–è®ºæ–‡è¯¦ç»†å†…å®¹"""
```

### Paperæ•°æ®æ¨¡å‹

```python
@dataclass
class Paper:
    arxiv_id: str                    # ArXiv ID
    title: str                       # è®ºæ–‡æ ‡é¢˜
    authors: List[str]               # ä½œè€…åˆ—è¡¨
    abstract: str                    # æ‘˜è¦
    subjects: List[str]              # å­¦ç§‘åˆ—è¡¨
    comments: str                    # è¯„è®º
    abs_link: str                    # æ‘˜è¦é“¾æ¥
    pdf_link: str                    # PDFé“¾æ¥
    html_link: str                    # HTMLé“¾æ¥
    submission_date: Optional[datetime] # æäº¤æ—¥æœŸ
    full_content: Optional[Dict]     # è¯¦ç»†å†…å®¹
    
    @property
    def primary_subject(self) -> str:
        """ä¸»è¦å­¦ç§‘"""
    
    @property
    def has_pdf(self) -> bool:
        """æ˜¯å¦æœ‰PDFç‰ˆæœ¬"""
    
    @property
    def has_html(self) -> bool:
        """æ˜¯å¦æœ‰HTMLç‰ˆæœ¬"""
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
```

### OutputFormatterç±»

```python
class OutputFormatter:
    def __init__(self, enable_rich: Optional[bool] = None):
        """åˆå§‹åŒ–è¾“å‡ºæ ¼å¼åŒ–å™¨"""
    
    def print_papers_table(self, papers: List[Paper], title: str = "è®ºæ–‡åˆ—è¡¨"):
        """æ‰“å°è®ºæ–‡è¡¨æ ¼"""
    
    def print_paper_detail(self, paper: Paper):
        """æ‰“å°å•ä¸ªè®ºæ–‡è¯¦æƒ…"""
    
    def print_paper_content(self, content: PaperContent):
        """æ‰“å°è®ºæ–‡è¯¦ç»†å†…å®¹"""
    
    def print_statistics(self, stats: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    
    def print_critical(self, *args, **kwargs):
        """æ‰“å°å…³é”®ä¿¡æ¯ï¼ˆå³ä½¿åœ¨é™é»˜æ¨¡å¼ä¸‹ä¹Ÿä¼šè¾“å‡ºï¼‰"""
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è§£æå™¨

```python
from parsers.html_parser import ArxivHtmlParser

class CustomParser(ArxivHtmlParser):
    def parse_custom_field(self, soup):
        """è‡ªå®šä¹‰è§£æé€»è¾‘"""
        # å®ç°è‡ªå®šä¹‰è§£æ
        pass

# ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨
scraper = ArxivScraper()
scraper.parser = CustomParser()
```

### æ‰¹é‡å¤„ç†å’Œå¯¼å‡º

```python
import json
import csv

# æ‰¹é‡è·å–å¹¶å¯¼å‡ºä¸ºJSON
papers = scraper.get_papers_from_category("cs_recent", max_papers=100)
papers_data = [paper.to_dict() for paper in papers]

with open("papers.json", "w", encoding="utf-8") as f:
    json.dump(papers_data, f, ensure_ascii=False, indent=2)

# å¯¼å‡ºä¸ºCSV
with open("papers.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=papers_data[0].keys())
    writer.writeheader()
    writer.writerows(papers_data)
```

### é”™è¯¯å¤„ç†å’Œæ—¥å¿—

```python
import logging
from core.scraper import ArxivScraper

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

try:
    with ArxivScraper() as scraper:
        papers = scraper.get_papers_from_category("cs_recent", max_papers=10)
        print(f"æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
        
except Exception as e:
    logging.error(f"è·å–è®ºæ–‡å¤±è´¥: {e}")
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. å­¦æœ¯ç ”ç©¶
```python
# è·å–ç‰¹å®šé¢†åŸŸçš„æœ€æ–°è®ºæ–‡
papers = scraper.get_papers_from_category("cs_recent", include_abstract=True)
ml_papers = [p for p in papers if "machine learning" in p.abstract.lower()]
```

### 2. æ•°æ®åˆ†æ
```python
# åˆ†æè®ºæ–‡ä½œè€…åˆ†å¸ƒ
from collections import Counter

all_authors = []
for paper in papers:
    all_authors.extend(paper.authors)

author_counts = Counter(all_authors)
top_authors = author_counts.most_common(10)
```

### 3. è‡ªåŠ¨åŒ–è„šæœ¬
```bash
# ä½¿ç”¨é™é»˜æ¨¡å¼è¿›è¡Œè‡ªåŠ¨åŒ–
export ARXIV_QUIET=true
python your_script.py > daily_papers.txt
```


## ğŸ§ª æµ‹è¯•

è¿è¡Œå…¨é¢æµ‹è¯•ï¼š

```bash
python comprehensive_test.py
```

æµ‹è¯•ç‰¹å®šåŠŸèƒ½ï¼š

```bash
# æµ‹è¯•é™é»˜æ¨¡å¼
python -c "
from arxiv_scraper.utils import OutputFormatter
formatter = OutputFormatter()
formatter.quiet_mode = True
print('æµ‹è¯•é™é»˜æ¨¡å¼')
"

# æµ‹è¯•ç¯å¢ƒå˜é‡
ARXIV_QUIET=true python -c "
from arxiv_scraper.utils import OutputFormatter
formatter = OutputFormatter()
print(f'é™é»˜æ¨¡å¼: {formatter.quiet_mode}')
"
```

## ğŸ“‹ ä¾èµ–

- **Python 3.8+**
- **requests** >= 2.28.0 - HTTPè¯·æ±‚
- **beautifulsoup4** >= 4.11.0 - HTMLè§£æ
- **rich** >= 12.0.0 - ç¾åŒ–è¾“å‡ºï¼ˆå¯é€‰ï¼‰
- **pandas** >= 1.5.0 - æ•°æ®å¤„ç†ï¼ˆå¯é€‰ï¼‰

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

- è¯·éµå®ˆArXivçš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txt
- å»ºè®®åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ é€‚å½“çš„å»¶è¿Ÿ
- å¤§æ‰¹é‡æŠ“å–æ—¶è¯·ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å¼ä»¥èŠ‚çœå†…å­˜
- å°Šé‡æœåŠ¡å™¨èµ„æºï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚

## ğŸ”— ç›¸å…³é“¾æ¥

- [ArXivå®˜ç½‘](https://arxiv.org/)
- [ArXiv APIæ–‡æ¡£](https://arxiv.org/help/api)
- [Richæ–‡æ¡£](https://rich.readthedocs.io/)

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒä¸€ä¸ªâ­ï¸ï¼**

Made with â¤ï¸ by XeanYu

</div> 